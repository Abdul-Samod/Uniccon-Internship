{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from googletrans import Translator\n",
        "\n",
        "# This command ensures the required libraries are installed before the script runs.\n",
        "# It is a standard practice for creating a self-contained script.\n",
        "try:\n",
        "    print(\"Installing required dependencies...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"googletrans==4.0.0rc1\", \"requests\", \"beautifulsoup4\", \"pandas\"])\n",
        "    print(\"Installation complete. Proceeding with scraping and translation...\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error during installation: {e}\", file=sys.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "# Headers to mimic a web browser\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "}\n",
        "\n",
        "# Define the topic and keywords you want to scrape for\n",
        "TARGET_TOPIC = 'finance'\n",
        "TARGET_KEYWORDS = ['owo', 'aje', 'okowo']\n",
        "\n",
        "BASE_URL = 'https://www.bbc.com/yoruba'\n",
        "translator = Translator()\n",
        "\n",
        "def translate_text(text):\n",
        "    \"\"\"Translates text using the googletrans library.\"\"\"\n",
        "    # Split text for translation to avoid API limits on long texts\n",
        "    chunks = [text[i:i + 4000] for i in range(0, len(text), 4000)]\n",
        "    translated_chunks = []\n",
        "    retries = 3\n",
        "    for chunk in chunks:\n",
        "        for i in range(retries):\n",
        "            try:\n",
        "                translated_chunks.append(translator.translate(chunk, src='yo', dest='en').text)\n",
        "                time.sleep(1) # Be respectful of the API\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if i < retries - 1:\n",
        "                    print(f\"Translation error: {e}. Retrying in {2**(i+1)} seconds...\", file=sys.stderr)\n",
        "                    time.sleep(2**(i+1))\n",
        "                else:\n",
        "                    print(f\"Translation failed after {retries} attempts for a chunk.\", file=sys.stderr)\n",
        "                    translated_chunks.append('Translation not available.')\n",
        "    return \" \".join(translated_chunks)\n",
        "\n",
        "def get_article_content(url):\n",
        "    \"\"\"Fetches and extracts the title and body text from a single article URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        title_element = soup.find('h1')\n",
        "        title = title_element.get_text(strip=True) if title_element else 'No Title'\n",
        "\n",
        "        article_body = soup.find('article') or soup.find('main') or soup.find('div', role='main')\n",
        "        if not article_body:\n",
        "            print(f\"Could not find main article body for {url}\", file=sys.stderr)\n",
        "            return title, None\n",
        "\n",
        "        body_parts = article_body.find_all('p')\n",
        "        article_text = ' '.join([p.get_text() for p in body_parts])\n",
        "\n",
        "        article_text = re.sub(r'\\s+', ' ', article_text).strip()\n",
        "\n",
        "        return title, article_text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching article at {url}: {e}\", file=sys.stderr)\n",
        "        return None, None\n",
        "\n",
        "def scrape_bbc_yoruba(topic, keywords):\n",
        "    \"\"\"Scrapes BBC Yoruba for articles based on keywords.\"\"\"\n",
        "    base_url = 'https://www.bbc.com/yoruba'\n",
        "    data = []\n",
        "\n",
        "    try:\n",
        "        print(f\"Fetching articles from {base_url}...\")\n",
        "        response = requests.get(base_url, headers=HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        links = soup.find_all('a', {'href': True})\n",
        "\n",
        "        article_urls = set()\n",
        "        for link in links:\n",
        "            href = link['href']\n",
        "            if '/yoruba/' in href and href.startswith('https://www.bbc.com/yoruba/'):\n",
        "                article_urls.add(href)\n",
        "\n",
        "        # Filter the URLs based on the keywords in the link text or URL itself\n",
        "        filtered_urls = [\n",
        "            url for url in list(article_urls)[:50]\n",
        "            if any(keyword in url.lower() or keyword in requests.get(url, headers=HEADERS).text.lower() for keyword in keywords)\n",
        "        ]\n",
        "\n",
        "        print(f\"Found {len(filtered_urls)} {topic}-related articles. Scraping...\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "            future_to_url = {executor.submit(get_article_content, url): url for url in filtered_urls}\n",
        "            for i, future in enumerate(as_completed(future_to_url)):\n",
        "                article_url = future_to_url[future]\n",
        "                print(f\"Processing article {i+1}/{len(filtered_urls)}: {article_url}\")\n",
        "                try:\n",
        "                    yoruba_title, article_content_yoruba = future.result()\n",
        "\n",
        "                    if article_content_yoruba and len(article_content_yoruba) > 20:\n",
        "                        data.append({\n",
        "                            'topic': topic,\n",
        "                            'yoruba_title': yoruba_title,\n",
        "                            'english_title': translate_text(yoruba_title),\n",
        "                            'yoruba_content': article_content_yoruba,\n",
        "                            'english_content': translate_text(article_content_yoruba),\n",
        "                            'url': article_url\n",
        "                        })\n",
        "                    else:\n",
        "                        print(f\"Skipping article at {article_url} due to empty content.\", file=sys.stderr)\n",
        "                except Exception as exc:\n",
        "                    print(f'{article_url} generated an exception: {exc}', file=sys.stderr)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the main page: {e}\", file=sys.stderr)\n",
        "        return\n",
        "\n",
        "    if data:\n",
        "        df = pd.DataFrame(data)\n",
        "        output_file = f'yoruba_{topic}_datasets.csv'\n",
        "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "        print(f\"\\n--- Successfully scraped and saved data to '{output_file}' ---\")\n",
        "    else:\n",
        "        print(f\"\\nNo data was scraped for {topic}. Check the website structure or your internet connection.\", file=sys.stderr)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    scrape_bbc_yoruba(TARGET_TOPIC, TARGET_KEYWORDS)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required dependencies...\n",
            "Installation complete. Proceeding with scraping and translation...\n",
            "Fetching articles from https://www.bbc.com/yoruba...\n",
            "Found 49 finance-related articles. Scraping...\n",
            "Processing article 1/49: https://www.bbc.com/yoruba/articles/c05e4djvzlro\n",
            "Processing article 2/49: https://www.bbc.com/yoruba/articles/ceq2xq5rw70o\n",
            "Processing article 3/49: https://www.bbc.com/yoruba/articles/c7v13lj65rzo\n",
            "Processing article 4/49: https://www.bbc.com/yoruba/articles/cx2p061mv3jo\n",
            "Processing article 5/49: https://www.bbc.com/yoruba/articles/cd07jj2xe1ko\n",
            "Processing article 6/49: https://www.bbc.com/yoruba/articles/c98l8dg40rdo\n",
            "Processing article 7/49: https://www.bbc.com/yoruba/articles/cwyrxq4d0r7o\n",
            "Processing article 8/49: https://www.bbc.com/yoruba/articles/cger8wp4zqyo\n",
            "Processing article 9/49: https://www.bbc.com/yoruba/articles/c15lpe11qwwo\n",
            "Processing article 10/49: https://www.bbc.com/yoruba/articles/cdd3lj97pddo\n",
            "Processing article 11/49: https://www.bbc.com/yoruba/articles/cwyw7e87nppo\n",
            "Processing article 12/49: https://www.bbc.com/yoruba/articles/crevejvy157o\n",
            "Processing article 13/49: https://www.bbc.com/yoruba/articles/cx2xjr7jkj7o\n",
            "Processing article 14/49: https://www.bbc.com/yoruba/institutional-48528718\n",
            "Processing article 15/49: https://www.bbc.com/yoruba/articles/cy04ge6d73jo\n",
            "Processing article 16/49: https://www.bbc.com/yoruba/articles/clylv8g20g9o\n",
            "Processing article 17/49: https://www.bbc.com/yoruba/articles/cz0yj95y54zo\n",
            "Processing article 18/49: https://www.bbc.com/yoruba/articles/cx2xyr1ydvxo\n",
            "Processing article 19/49: https://www.bbc.com/yoruba/articles/crm4410dylro\n",
            "Processing article 20/49: https://www.bbc.com/yoruba/articles/cddm90rl1ylo\n",
            "Processing article 21/49: https://www.bbc.com/yoruba/articles/c8xrq4zey0lo\n",
            "Processing article 22/49: https://www.bbc.com/yoruba/articles/c20v4znzpwzo\n",
            "Processing article 23/49: https://www.bbc.com/yoruba/articles/cm2dv97zxdxo\n",
            "Processing article 24/49: https://www.bbc.com/yoruba/articles/c87e991y5rro\n",
            "Processing article 25/49: https://www.bbc.com/yoruba/topics/c340q0y3p5kt\n",
            "Processing article 26/49: https://www.bbc.com/yoruba/articles/cq65g2ndqmlo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Skipping article at https://www.bbc.com/yoruba/topics/c340q0y3p5kt due to empty content.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing article 27/49: https://www.bbc.com/yoruba/articles/c20v3jxzzepo\n",
            "Processing article 28/49: https://www.bbc.com/yoruba/articles/c0qlxp5lxj0o\n",
            "Processing article 29/49: https://www.bbc.com/yoruba/articles/c931lv3ge4wo\n",
            "Processing article 30/49: https://www.bbc.com/yoruba/articles/cn4lzxg14wro\n",
            "Processing article 31/49: https://www.bbc.com/yoruba/articles/cx2x59n4d4jo\n",
            "Processing article 32/49: https://www.bbc.com/yoruba/articles/cpv09ejglypo\n",
            "Processing article 33/49: https://www.bbc.com/yoruba/articles/c3vzx66qn30o\n",
            "Processing article 34/49: https://www.bbc.com/yoruba/articles/ckg2v8jwd47o\n",
            "Processing article 35/49: https://www.bbc.com/yoruba/articles/cr4qlygvewxo\n",
            "Processing article 36/49: https://www.bbc.com/yoruba/articles/cpwyg59jpzjo\n",
            "Processing article 37/49: https://www.bbc.com/yoruba/topics/ck5rznlk6k3t\n",
            "Processing article 38/49: https://www.bbc.com/yoruba/articles/cdxy704qe0zo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Skipping article at https://www.bbc.com/yoruba/topics/ck5rznlk6k3t due to empty content.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing article 39/49: https://www.bbc.com/yoruba/articles/czrp71rkv22o\n",
            "Processing article 40/49: https://www.bbc.com/yoruba/articles/cwyw90gdxrro\n",
            "Processing article 41/49: https://www.bbc.com/yoruba/articles/c9qyq3g103no\n",
            "Processing article 42/49: https://www.bbc.com/yoruba/articles/ceq2x43dwn9o\n",
            "Processing article 43/49: https://www.bbc.com/yoruba/articles/c7542p2qe7xo\n",
            "Processing article 44/49: https://www.bbc.com/yoruba/articles/cdr6lp4r258o\n",
            "Processing article 45/49: https://www.bbc.com/yoruba/articles/c4gl49q0qp7o\n",
            "Processing article 46/49: https://www.bbc.com/yoruba/articles/cjr1r55821zo\n",
            "Processing article 47/49: https://www.bbc.com/yoruba/articles/c8x520v41gdo\n",
            "Processing article 48/49: https://www.bbc.com/yoruba/articles/cdr60zrg6m1o\n",
            "Processing article 49/49: https://www.bbc.com/yoruba/articles/cpd9vdneqn9o\n",
            "\n",
            "--- Successfully scraped and saved data to 'yoruba_finance_datasets.csv' ---\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeyCbVKEbbA2",
        "outputId": "a267b054-8672-41e2-b9e7-db9b1b86a16b"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}